"""
Script 3: Multi-Modal LoRA Exchange with OpenSource Models
==========================================================

This script demonstrates LoRA adapter exchange between different open-source
vision-language models including BLIP-2, InstructBLIP, and LLaVA.
"""

import torch
import warnings
warnings.filterwarnings("ignore")

from transformers import (
    Blip2Processor, Blip2ForConditionalGeneration,
    InstructBlipProcessor, InstructBlipForConditionalGeneration,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, PeftModel, TaskType
from datasets import load_dataset, Dataset
from PIL import Image
import requests
from typing import Dict, List, Optional
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import json
import os

class MultiModalLoRAExchange:
    """LoRA Exchange System for Multiple Vision-Language Models"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.processors = {}
        self.adapters = {}
        self.adapter_metadata = {}
        self.datasets = {}
        
        print(f"üîÑ Initializing Multi-Modal LoRA Exchange System")
        print(f"Device: {self.device}")
        
        self._load_models()
        self._setup_adapter_storage()
        self._load_multimodal_datasets()
    
    def _load_models(self):
        """Load multiple vision-language models"""
        try:
            # Memory optimization config
            if self.device.type == "cuda":
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # 1. BLIP-2 with OPT-2.7B
            print("Loading BLIP-2 OPT-2.7B...")
            self.processors["blip2_opt"] = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
            
            if self.device.type == "cuda":
                self.models["blip2_opt"] = Blip2ForConditionalGeneration.from_pretrained(
                    "Salesforce/blip2-opt-2.7b",
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16
                )
            else:
                self.models["blip2_opt"] = Blip2ForConditionalGeneration.from_pretrained(
                    "Salesforce/blip2-opt-2.7b",
                    torch_dtype=torch.float32
                )
                self.models["blip2_opt"].to(self.device)
            
            print("‚úì BLIP-2 OPT loaded")
            
            # 2. BLIP-2 with FLAN-T5
            print("Loading BLIP-2 FLAN-T5...")
            self.processors["blip2_flan"] = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
            
            if self.device.type == "cuda":
                self.models["blip2_flan"] = Blip2ForConditionalGeneration.from_pretrained(
                    "Salesforce/blip2-flan-t5-xl",
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16
                )
            else:
                self.models["blip2_flan"] = Blip2ForConditionalGeneration.from_pretrained(
                    "Salesforce/blip2-flan-t5-xl",
                    torch_dtype=torch.float32
                )
                self.models["blip2_flan"].to(self.device)
            
            print("‚úì BLIP-2 FLAN-T5 loaded")
            
            # 3. InstructBLIP (instruction-following BLIP)
            try:
                print("Loading InstructBLIP...")
                self.processors["instructblip"] = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-vicuna-7b")
                
                if self.device.type == "cuda":
                    self.models["instructblip"] = InstructBlipForConditionalGeneration.from_pretrained(
                        "Salesforce/instructblip-vicuna-7b",
                        quantization_config=bnb_config,
                        device_map="auto",
                        torch_dtype=torch.float16
                    )
                else:
                    # InstructBLIP is quite large, skip on CPU
                    print("‚ö†Ô∏è  Skipping InstructBLIP on CPU (too large)")
                
                if "instructblip" in self.models:
                    print("‚úì InstructBLIP loaded")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  InstructBLIP loading failed: {e}")
            
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")
            raise
    
    def _setup_adapter_storage(self):
        """Setup adapter storage directory"""
        self.adapter_dir = "lora_adapters"
        os.makedirs(self.adapter_dir, exist_ok=True)
        
        # Load existing adapter metadata
        metadata_file = os.path.join(self.adapter_dir, "metadata.json")
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                self.adapter_metadata = json.load(f)
    
    def _load_multimodal_datasets(self):
        """Load comprehensive multimodal datasets"""
        print("üìö Loading comprehensive multimodal datasets...")
        
        try:
            # 1. Load Visual Dialog dataset for multiple tasks
            print("Loading Visual Dialog dataset...")
            try:
                visdial_dataset = load_dataset("jxu124/visdial", split="train[:1200]")
                
                # Use Visual Dialog for multiple task types
                self.datasets["vqa"] = self._preprocess_visdial_for_vqa(visdial_dataset)
                self.datasets["instruction_following"] = self._preprocess_visdial_for_instructions(visdial_dataset)
                self.datasets["complex_reasoning"] = self._preprocess_visdial_for_reasoning(visdial_dataset)
                
                print(f"‚úì Loaded Visual Dialog - VQA: {len(self.datasets['vqa'])}")
                print(f"‚úì Loaded Visual Dialog - Instructions: {len(self.datasets['instruction_following'])}")
                print(f"‚úì Loaded Visual Dialog - Reasoning: {len(self.datasets['complex_reasoning'])}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Visual Dialog loading failed: {e}")
                # Fall back to synthetic data
                self._create_fallback_datasets_with_data()
                return

            # 2. Load additional datasets for captioning
            print("Loading additional datasets for captioning...")
            try:
                # Try to get some captioning data
                cc_dataset = load_dataset("conceptual_captions", split="train[:300]")
                self.datasets["captioning"] = self._preprocess_cc_for_captioning(cc_dataset)
                print(f"‚úì Loaded captioning samples: {len(self.datasets['captioning'])}")
            except Exception as e:
                print(f"‚ö†Ô∏è Captioning dataset failed: {e}")
                # Use Visual Dialog captions
                self.datasets["captioning"] = self._preprocess_visdial_for_captioning(visdial_dataset)
                print(f"‚úì Used Visual Dialog for captioning: {len(self.datasets['captioning'])}")

            # 3. Create text-dense dataset (synthetic for now)
            print("Creating text-dense dataset...")
            self.datasets["text_dense"] = self._create_text_dense_from_visdial()
            print(f"‚úì Created text-dense samples: {len(self.datasets['text_dense'])}")
            
        except Exception as e:
            print(f"‚ùå Error loading datasets: {e}")
            self._create_fallback_datasets()

    def _preprocess_visdial_for_vqa(self, dataset):
        """Preprocess Visual Dialog for VQA tasks"""
        processed_data = []
        
        for item in dataset:
            try:
                if 'image' in item and 'dialog' in item:
                    image = item['image']
                    dialog = item['dialog']
                    
                    # Use first 2 QA pairs from each dialog
                    for qa_pair in dialog[:2]:
                        if 'question' in qa_pair and 'answer' in qa_pair:
                            processed_data.append({
                                'image': image,
                                'text': f"Question: {qa_pair['question']} Answer: {qa_pair['answer']}",

                                'task_type': 'vqa',
                                'instruction': qa_pair['question']
                            })
                            
            except Exception:
                continue
                
            if len(processed_data) >= 400:
                break
        
        return processed_data

    def _preprocess_visdial_for_instructions(self, dataset):
        """Preprocess Visual Dialog for instruction following"""
        processed_data = []
        
        for item in dataset:
            try:
                if 'image' in item and 'dialog' in item:
                    image = item['image']
                    caption = item.get('caption', '')
                    dialog = item['dialog']
                    
                    # Create instruction-following examples
                    if dialog and caption:
                        first_qa = dialog[0]
                        if 'question' in first_qa and 'answer' in first_qa:
                            instruction = f"Look at this image and answer: {first_qa['question']}"
                            response = f"Based on what I see, {first_qa['answer']}"
                            
                            processed_data.append({
                                'image': image,
                                'text': f"Instruction: {instruction} Response: {response}",

                                'task_type': 'instruction_following',
                                'instruction': instruction
                            })
                            
            except Exception:
                continue
                
            if len(processed_data) >= 300:
                break
        
        return processed_data

    def _preprocess_visdial_for_reasoning(self, dataset):
        """Preprocess Visual Dialog for complex reasoning"""
        processed_data = []
        
        reasoning_keywords = ['why', 'how', 'because', 'explain', 'reason']
        
        for item in dataset:
            try:
                if 'image' in item and 'dialog' in item:
                    image = item['image']
                    dialog = item['dialog']
                    
                    # Look for reasoning questions
                    for qa_pair in dialog:
                        if 'question' in qa_pair and 'answer' in qa_pair:
                            question = qa_pair['question'].lower()
                            
                            if any(keyword in question for keyword in reasoning_keywords):
                                processed_data.append({
                                    'image': image,
                                    'text': f"Question: {qa_pair['question']} Answer: {qa_pair['answer']}",

                                    'task_type': 'complex_reasoning',
                                    'instruction': qa_pair['question']
                                })
                                
            except Exception:
                continue
                
            if len(processed_data) >= 200:
                break
        
        return processed_data

    def _preprocess_visdial_for_captioning(self, dataset):
        """Use Visual Dialog captions for captioning task"""
        processed_data = []
        
        for item in dataset:
            try:
                if 'image' in item and 'caption' in item:
                    image = item['image']
                    caption = item['caption']
                    
                    processed_data.append({
                        'image': image,
                        'text': caption,
                        'task_type': 'captioning',
                        'instruction': 'Describe this image in detail.'
                    })
                    
            except Exception:
                continue
                
            if len(processed_data) >= 300:
                break
        
        return processed_data

    def _preprocess_cc_for_captioning(self, dataset):
        """Preprocess Conceptual Captions for captioning task"""
        processed_data = []
        
        # Create sample image to avoid download issues
        from PIL import Image
        import numpy as np
        sample_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        
        for item in dataset:
            try:
                if 'caption' in item:
                    caption = item['caption']
                    processed_data.append({
                        'image': sample_image,
                        'text': caption,
                        'task_type': 'captioning',
                        'instruction': 'Describe this image.'
                    })
                    
            except Exception:
                continue
                
            if len(processed_data) >= 200:
                break
        
        return processed_data

    def _create_text_dense_from_visdial(self):
        """Create text-dense reading tasks"""
        # Create synthetic text-dense examples
        from PIL import Image
        import numpy as np
        sample_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        
        text_dense_examples = [
            {
                'image': sample_image,
                'text': 'Read the text in the image and answer: What does the sign say? Answer: The sign contains various text elements.',
                'task_type': 'text_dense',
                'instruction': 'What text do you see?'
            }
        ] * 50
        
        return text_dense_examples

    def create_adapter(self, model_name: str, adapter_name: str, 
                      task_specialization: str = "general", 
                      complexity: str = "medium") -> str:
        """Create a LoRA adapter for specific model and task"""
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available")
        
        print(f"Creating adapter '{adapter_name}' for {model_name}")
        print(f"  Task: {task_specialization}")
        print(f"  Complexity: {complexity}")
        
        # Configure LoRA parameters based on complexity and task
        if complexity == "light":
            r, alpha = 4, 8
            dropout = 0.05
        elif complexity == "medium":
            r, alpha = 8, 16
            dropout = 0.1
        else:  # heavy
            r, alpha = 16, 32
            dropout = 0.1
        
        # Fix target modules based on model architecture
        if "flan" in model_name:
            # FLAN-T5 has different module names
            if task_specialization == "captioning":
                target_modules = ["q", "v", "k", "o"]
            elif task_specialization == "vqa":
                target_modules = ["q", "v", "k", "o", "wi_0", "wi_1"]
            elif task_specialization == "instruction_following":
                target_modules = ["q", "v", "k", "o", "wi_0", "wi_1"]
            elif task_specialization == "complex_reasoning":
                target_modules = ["q", "v", "k", "o", "wi_0", "wi_1", "wo"]
            else:  # general
                target_modules = ["q", "v", "k", "o"]
        else:
            # OPT and other models use standard names
            if task_specialization == "captioning":
                target_modules = ["q_proj", "v_proj", "k_proj", "out_proj"]
            elif task_specialization == "vqa":
                target_modules = ["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
            elif task_specialization == "instruction_following":
                target_modules = ["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
            elif task_specialization == "complex_reasoning":
                target_modules = ["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
            else:  # general
                target_modules = ["q_proj", "v_proj", "k_proj", "out_proj"]
        
        lora_config = LoraConfig(
            r=r,
            lora_alpha=alpha,
            target_modules=target_modules,
            lora_dropout=dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # Create PEFT model
        base_model = self.models[model_name]
        peft_model = get_peft_model(base_model, lora_config)
        
        # Store adapter
        adapter_key = f"{model_name}_{adapter_name}"
        self.adapters[adapter_key] = peft_model
        
        # Fix parameter count handling
        try:
            trainable_params = peft_model.get_nb_trainable_parameters()
            if isinstance(trainable_params, tuple):
                trainable_count = trainable_params[0]
                total_count = trainable_params[1]
            else:
                trainable_count = trainable_params
                total_count = "unknown"
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get parameter count: {e}")
            trainable_count = "unknown"
            total_count = "unknown"
        
        # Save metadata
        self.adapter_metadata[adapter_key] = {
            "model_name": model_name,
            "adapter_name": adapter_name,
            "task_specialization": task_specialization,
            "complexity": complexity,
            "lora_r": r,
            "lora_alpha": alpha,
            "trainable_params": trainable_count,
            "total_params": total_count,
            "target_modules": target_modules
        }
        
        print(f"‚úì Created adapter with trainable parameters: {trainable_count}")
        return adapter_key
    
    def train_adapter_with_dataset(self, model_name: str, adapter_name: str, 
                                 task_specialization: str, epochs: int = 3, batch_size: int = 4):
        """Train adapter with real multimodal dataset"""
        adapter_key = f"{model_name}_{adapter_name}"
        if adapter_key not in self.adapters:
            raise ValueError(f"Adapter {adapter_key} not found")
        
        dataset = self.datasets.get(task_specialization, [])
        if not dataset:
            print(f"‚ùå No dataset available for task: {task_specialization}")
            return
        
        print(f"üéØ Training {adapter_key} with {len(dataset)} {task_specialization} examples...")
        
        model = self.adapters[adapter_key]
        processor = self.processors[model_name]
        model.train()
        
        # Prepare optimizer
        optimizer = AdamW(model.parameters(), lr=5e-5)
        
        # Create dataloader
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        total_loss = 0
        step = 0
        
        for epoch in range(epochs):
            print(f"Epoch {epoch + 1}/{epochs}")
            epoch_loss = 0
            
            for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=f"Training {task_specialization}")):
                try:
                    # Prepare batch
                    images = []
                    texts = []
                    
                    for item in batch_data:
                        if item['image'] is not None:
                            images.append(item['image'])
                            texts.append(item['text'])
                    
                    if not images:
                        continue
                    
                    # Prepare inputs based on model type
                    if "instruct" in model_name:
                        # InstructBLIP format
                        inputs = processor(images=images, text=texts, return_tensors="pt", 
                                         padding=True, truncation=True)
                    else:
                        # BLIP-2 format
                        inputs = processor(images=images, text=texts, return_tensors="pt", 
                                         padding=True, truncation=True)
                    
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    # Forward pass
                    optimizer.zero_grad()
                    outputs = model(**inputs, labels=inputs["input_ids"])
                    loss = outputs.loss
                    
                    # Backward pass
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    total_loss += loss.item()
                    step += 1
                    
                    if batch_idx % 10 == 0:
                        print(f"  Batch {batch_idx}, Loss: {loss.item():.4f}")
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Batch {batch_idx} failed: {e}")
                    continue
            
            avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0
            print(f"  Epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}")
        
        avg_total_loss = total_loss / step if step > 0 else 0
        print(f"‚úì Training completed. Average loss: {avg_total_loss:.4f}")
        
        # Save trained adapter
        adapter_path = os.path.join(self.adapter_dir, adapter_key)
        os.makedirs(adapter_path, exist_ok=True)
        model.save_pretrained(adapter_path)
        
        # Update metadata
        self.adapter_metadata[adapter_key]["trained"] = True
        self.adapter_metadata[adapter_key]["training_loss"] = avg_total_loss
        self._save_metadata()
        
        print(f"‚úì Saved trained adapter to {adapter_path}")
    
    def _save_metadata(self):
        """Save adapter metadata"""
        metadata_file = os.path.join(self.adapter_dir, "metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(self.adapter_metadata, f, indent=2)
    
    def save_adapter(self, adapter_key: str) -> str:
        """Save adapter to disk"""
        if adapter_key not in self.adapters:
            raise ValueError(f"Adapter {adapter_key} not found")
        
        adapter_path = os.path.join(self.adapter_dir, adapter_key)
        os.makedirs(adapter_path, exist_ok=True)
        
        # Save adapter weights
        self.adapters[adapter_key].save_pretrained(adapter_path)
        
        # Update metadata file
        metadata_file = os.path.join(self.adapter_dir, "metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(self.adapter_metadata, f, indent=2)
        
        print(f"‚úì Saved adapter {adapter_key} to {adapter_path}")
        return adapter_path
    
    def load_adapter(self, adapter_key: str, model_name: str) -> bool:
        """Load adapter from disk"""
        adapter_path = os.path.join(self.adapter_dir, adapter_key)
        
        if not os.path.exists(adapter_path):
            print(f"‚ùå Adapter {adapter_key} not found at {adapter_path}")
            return False
        
        try:
            base_model = self.models[model_name]
            peft_model = PeftModel.from_pretrained(base_model, adapter_path)
            self.adapters[adapter_key] = peft_model
            
            print(f"‚úì Loaded adapter {adapter_key}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to load adapter {adapter_key}: {e}")
            return False
    
    def exchange_adapters(self, adapter1_key: str, adapter2_key: str) -> Dict:
        """Exchange/merge two adapters"""
        if adapter1_key not in self.adapters or adapter2_key not in self.adapters:
            raise ValueError("One or both adapters not found")
        
        print(f"üîÑ Exchanging adapters: {adapter1_key} ‚Üî {adapter2_key}")
        
        # Get adapter information
        adapter1_info = self.adapter_metadata[adapter1_key]
        adapter2_info = self.adapter_metadata[adapter2_key]
        
        # Create exchange summary
        exchange_info = {
            "adapter1": {
                "key": adapter1_key,
                "task": adapter1_info["task_specialization"],
                "complexity": adapter1_info["complexity"],
                "params": adapter1_info["trainable_params"]
            },
            "adapter2": {
                "key": adapter2_key,
                "task": adapter2_info["task_specialization"],
                "complexity": adapter2_info["complexity"],
                "params": adapter2_info["trainable_params"]
            },
            "compatibility": self._check_compatibility(adapter1_info, adapter2_info)
        }
        
        print(f"  Adapter 1: {adapter1_info['task_specialization']} ({adapter1_info['complexity']})")
        print(f"  Adapter 2: {adapter2_info['task_specialization']} ({adapter2_info['complexity']})")
        print(f"  Compatibility: {exchange_info['compatibility']}")
        
        return exchange_info
    
    def _check_compatibility(self, adapter1_info: Dict, adapter2_info: Dict) -> str:
        """Check compatibility between two adapters"""
        if adapter1_info["model_name"] != adapter2_info["model_name"]:
            return "incompatible_models"
        
        if adapter1_info["lora_r"] == adapter2_info["lora_r"]:
            return "fully_compatible"
        
        if abs(adapter1_info["lora_r"] - adapter2_info["lora_r"]) <= 4:
            return "mostly_compatible"
        
        return "parameter_mismatch"
    
    def cross_model_adapter_transfer(self, source_adapter: str, target_model: str, 
                                   new_adapter_name: str) -> bool:
        """Transfer adapter from one model to another (experimental)"""
        if source_adapter not in self.adapters:
            print(f"‚ùå Source adapter {source_adapter} not found")
            return False
        
        if target_model not in self.models:
            print(f"‚ùå Target model {target_model} not found")
            return False
        
        print(f"üîÑ Attempting cross-model transfer: {source_adapter} ‚Üí {target_model}")
        
        source_info = self.adapter_metadata[source_adapter]
        
        try:
            # Create new adapter for target model with similar config
            new_adapter_key = self.create_adapter(
                target_model, new_adapter_name, 
                source_info["task_specialization"],
                source_info["complexity"]
            )
            
            print(f"‚úì Created target adapter {new_adapter_key}")
            
            # Note: In practice, you would implement weight transfer logic here
            # For now, we just create a new adapter and mark it as transferred
            self.adapter_metadata[new_adapter_key]["transferred_from"] = source_adapter
            self._save_metadata()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Transfer failed: {e}")
            return False
    
    def generate_with_adapter(self, image_url: str, prompt: str, 
                            adapter_key: str, max_length: int = 50) -> str:
        """Generate text using specific adapter"""
        if adapter_key not in self.adapters:
            raise ValueError(f"Adapter {adapter_key} not found")
        
        try:
            # Get model info
            model_name = self.adapter_metadata[adapter_key]["model_name"]
            model = self.adapters[adapter_key]
            processor = self.processors[model_name]
            
            # Load image
            if image_url.startswith(('http://', 'https://')):
                image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')
            else:
                image = Image.open(image_url).convert('RGB')
            
            # Prepare inputs
            if "instruct" in model_name:
                # InstructBLIP uses different prompt format
                full_prompt = prompt
            else:
                # BLIP-2 format
                full_prompt = f"Question: {prompt} Answer:"
            
            inputs = processor(images=image, text=full_prompt, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Generate
            model.eval()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            # Decode
            result = processor.decode(outputs[0], skip_special_tokens=True)
            if "Answer:" in result:
                result = result.split("Answer:")[-1].strip()
            
            return result
            
        except Exception as e:
            return f"Error: {e}"
    
    def evaluate_adapter_performance(self, adapter_key: str, test_samples: int = 10):
        """Evaluate adapter performance on test data"""
        if adapter_key not in self.adapters:
            print(f"‚ùå Adapter {adapter_key} not found")
            return
        
        task_type = self.adapter_metadata[adapter_key]["task_specialization"]
        dataset = self.datasets.get(task_type, [])
        
        if len(dataset) < test_samples:
            test_samples = len(dataset)
        
        print(f"üîç Evaluating {adapter_key} on {test_samples} {task_type} samples...")
        
        test_data = dataset[-test_samples:]  # Use last samples as test
        
        for i, item in enumerate(test_data):
            if item['image'] is not None:
                result = self.generate_with_adapter("", item['instruction'], adapter_key)
                ground_truth = item['text'].split("Response:")[-1].strip() if "Response:" in item['text'] else item['text']
                
                print(f"Sample {i+1}:")
                print(f"  Instruction: {item['instruction']}")
                print(f"  Generated: {result}")
                print(f"  Expected: {ground_truth}")
                print()
    
    def list_adapters(self) -> Dict:
        """List all available adapters"""
        print(f"\nüìã Available Adapters ({len(self.adapter_metadata)})")
        print("-" * 60)
        
        for adapter_key, info in self.adapter_metadata.items():
            status = "loaded" if adapter_key in self.adapters else "saved"
            
            # Safe parameter formatting
            try:
                trainable_params = info.get('trainable_params', 0)
                if isinstance(trainable_params, (int, float)):
                    param_str = f"{int(trainable_params):,}"
                elif isinstance(trainable_params, tuple):
                    param_str = f"{int(trainable_params[0]):,}"
                else:
                    param_str = str(trainable_params)
            except Exception:
                param_str = "unknown"
            
            print(f"{adapter_key}:")
            print(f"  Model: {info['model_name']}")
            print(f"  Task: {info['task_specialization']}")
            print(f"  Complexity: {info['complexity']}")
            print(f"  Parameters: {param_str}")
            print(f"  Status: {status}")
            print()
        
        return self.adapter_metadata


def demo_multimodal_exchange():
    """Demonstrate multi-modal LoRA exchange with real datasets"""
    print("üîÑ Multi-Modal LoRA Exchange Demo with HuggingFace Datasets")
    print("=" * 70)
    
    try:
        # Initialize system
        exchange_system = MultiModalLoRAExchange()
        
        # Create various adapters for different tasks
        print("\nüõ†Ô∏è  Creating specialized adapters...")
        
        adapters_to_create = [
            ("blip2_opt", "image_caption", "captioning", "medium"),
            ("blip2_opt", "vqa_specialist", "vqa", "heavy"),
            ("blip2_flan", "reasoning_expert", "complex_reasoning", "heavy"),
            ("blip2_flan", "instruction_follower", "instruction_following", "medium")
        ]
        
        created_adapters = []
        for model_name, adapter_name, task, complexity in adapters_to_create:
            if model_name in exchange_system.models:
                try:
                    adapter_key = exchange_system.create_adapter(
                        model_name, adapter_name, task, complexity
                    )
                    created_adapters.append((adapter_key, model_name, adapter_name, task))
                except Exception as e:
                    print(f"‚ùå Failed to create {adapter_name}: {e}")
        
        # Train adapters with real datasets
        print(f"\nüéØ Training adapters with HuggingFace datasets...")
        
        for adapter_key, model_name, adapter_name, task in created_adapters:
            try:
                exchange_system.train_adapter_with_dataset(
                    model_name, adapter_name, task, epochs=2, batch_size=2
                )
                exchange_system.save_adapter(adapter_key)
            except Exception as e:
                print(f"‚ö†Ô∏è Training failed for {adapter_name}: {e}")
        
        # List all adapters
        exchange_system.list_adapters()
        
        # Test cross-model adapter transfer
        if len(created_adapters) >= 2:
            print(f"\nüîÑ Testing cross-model adapter transfer...")
            source_adapter = created_adapters[0][0]
            target_model = "blip2_flan"
            
            if target_model in exchange_system.models:
                success = exchange_system.cross_model_adapter_transfer(
                    source_adapter, target_model, "transferred_adapter"
                )
                if success:
                    print("‚úì Cross-model transfer successful")
        
        # Test generation and evaluation
        print(f"\nüéØ Testing generation with trained adapters...")
        
        test_image = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"
        test_prompts = [
            "Describe this image in detail",
            "What animals do you see?",
            "Analyze the composition of this image"
        ]
        
        for adapter_key, model_name, adapter_name, task in created_adapters[:2]:
            print(f"\nTesting {adapter_name} ({task}):")
            for prompt in test_prompts:
                try:
                    result = exchange_system.generate_with_adapter(
                        test_image, prompt, adapter_key
                    )
                    print(f"  '{prompt}' ‚Üí {result}")
                except Exception as e:
                    print(f"  '{prompt}' ‚Üí Error: {e}")
        
        # Evaluate adapter performance
        print(f"\nüìä Evaluating adapter performance...")
        
        for adapter_key, model_name, adapter_name, task in created_adapters:
            try:
                exchange_system.evaluate_adapter_performance(adapter_key, test_samples=3)
            except Exception as e:
                print(f"‚ö†Ô∏è Evaluation failed for {adapter_name}: {e}")
        
        print("\n" + "=" * 70)
        print("‚úÖ Multi-Modal LoRA Exchange Demo Complete!")
        print("=" * 70)
        
        return exchange_system
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        return None




if __name__ == "__main__":
    demo_multimodal_exchange()
